# Generic Feed Forward Neural Network
Developed python library implementing neural networks with support for custom number of hidden layers & neurons in each layer. Implemented back propogation from strach for updating the weight parameters of the model. We are given an MNIST dataset to train our model. The MNIST database is a large
database of handwritten digits that is commonly used for training various image processing systems. Studied various hyperparameters such as learning rate, batch size, activation function, number of hidden layers, number of neurons in each layer and their effects on the model performance.

## Resources
Chapter 6, Neural Networks - A Classroom Approach by Satish Kumar
Additional References:
1. [Understanding backpropagation](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)
2. [NNets and backpropagation](https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e)

Alternative Approach (Using Computation Graph):
1. [Backpropagation and Neural Networks using Computation Graph](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf)

Extra topics studied for the assignment
1. [Understanding Overfitting and underfitting using a complete example](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)
2. [Overfitting and Underfitting](https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690)
2. [Regularization](https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/)
